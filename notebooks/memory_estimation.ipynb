{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7b7089d84cd24d43835da8377039060e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4b41765a4e94d58a6c964b757705764",
              "IPY_MODEL_0109085e26004df9b3d938421a7233fb",
              "IPY_MODEL_a02ed34063204711a4a25a253f0d22cf"
            ],
            "layout": "IPY_MODEL_87102c6a1acd4e09b4c4b47378e352b1"
          }
        },
        "a4b41765a4e94d58a6c964b757705764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e51a4808dc44511bee1551a19c9bde4",
            "placeholder": "​",
            "style": "IPY_MODEL_66cd870bf1bb44718ac140591873e91d",
            "value": "config.json: 100%"
          }
        },
        "0109085e26004df9b3d938421a7233fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_675da569a57f4444bea3cabfe06acfb3",
            "max": 879,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33b59c6668444cf6832d48d1e361c77b",
            "value": 879
          }
        },
        "a02ed34063204711a4a25a253f0d22cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_635cda9b019942dcb990b2f20e8b36a7",
            "placeholder": "​",
            "style": "IPY_MODEL_c1e568eb59fb4286ba604568aea5faa4",
            "value": " 879/879 [00:00&lt;00:00, 49.6kB/s]"
          }
        },
        "87102c6a1acd4e09b4c4b47378e352b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e51a4808dc44511bee1551a19c9bde4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66cd870bf1bb44718ac140591873e91d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "675da569a57f4444bea3cabfe06acfb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33b59c6668444cf6832d48d1e361c77b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "635cda9b019942dcb990b2f20e8b36a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1e568eb59fb4286ba604568aea5faa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*More details in this article: [Estimating Memory Usage for LLMs During Inference (V2)](https://kaitchup.substack.com/p/estimating-memory-usage-for-llms)*\n",
        "\n",
        "This notebook estimates the memory consumption of transformer models for inference.\n",
        "\n",
        "This is only an approximation of the total memory consumed by the model, which applies optimizations like KV caching, FlashAttention, and GQA.\n",
        "\n",
        "To get the estimation, run all the cells.\n",
        "\n",
        "First, if you want to estimate the memory consumption of recent models, make sure you are using the last version of Hugging Face transformers.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zny8982jpQ9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdV3LUtH2YK1",
        "outputId": "919e27cf-9155-4eb5-aec7-10bc2c206cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
            "  Downloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed tokenizers-0.20.0 transformers-4.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the following interactive cell, enter the name of the model. It can be the name of the repository on the Hugging Face Hub or a local path.\n",
        "This cell retrieves the architecture of the model."
      ],
      "metadata": {
        "id": "u3wJXIeXpKTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.3-70B-Instruct\" # @param {type:\"string\"}\n",
        "\n",
        "model_config = AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "hidden_layers = model_config.num_hidden_layers\n",
        "hidden_size =  model_config.hidden_size\n",
        "attention_heads = model_config.num_attention_heads\n",
        "kv_heads = 0\n",
        "if hasattr(model_config, \"num_key_value_heads\"):\n",
        "  kv_heads = model_config.num_key_value_heads\n",
        "\n",
        "print(\"Model: \"+str(model_name))\n",
        "print(\"Hidden layers (L): \"+str(hidden_layers))\n",
        "print(\"Hidden size (h): \"+str(hidden_size))\n",
        "print(\"Attention heads (a): \"+str(attention_heads))\n",
        "if kv_heads > 0:\n",
        "  print(\"Key-value heads (g): \"+str(kv_heads))"
      ],
      "metadata": {
        "id": "PAaucRvmmf0F",
        "outputId": "2437923a-cf5c-433c-cbdc-dc52e4e99fd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "7b7089d84cd24d43835da8377039060e",
            "a4b41765a4e94d58a6c964b757705764",
            "0109085e26004df9b3d938421a7233fb",
            "a02ed34063204711a4a25a253f0d22cf",
            "87102c6a1acd4e09b4c4b47378e352b1",
            "5e51a4808dc44511bee1551a19c9bde4",
            "66cd870bf1bb44718ac140591873e91d",
            "675da569a57f4444bea3cabfe06acfb3",
            "33b59c6668444cf6832d48d1e361c77b",
            "635cda9b019942dcb990b2f20e8b36a7",
            "c1e568eb59fb4286ba604568aea5faa4"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/879 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b7089d84cd24d43835da8377039060e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: meta-llama/Llama-3.3-70B-Instruct\n",
            "Hidden layers (L): 80\n",
            "Hidden size (h): 8192\n",
            "Attention heads (a): 64\n",
            "Key-value heads (g): 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following interactive cell enter:\n",
        "- nb_billion_parameter: the number of parameters in the model, in billions. For instance, for Llama 3 8B enter 8.03 since the model has 8.03 billion parameters.\n",
        "- bitwidth_model: The number of bits per parameters. For instance 16, if you load the model with float16 or bfloat16.\n",
        "- seqlen: The maximum sequence length in your batches.\n",
        "- batch_size: The number of instances in one batch."
      ],
      "metadata": {
        "id": "rWkpPCD0GLuo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMAwju0nKh3r",
        "outputId": "7cb3b3a1-47ce-475c-e8df-7da699bd73fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters in the model (n): 70.6B\n",
            "Bitwidth of the model's parameters (p): 16-bit\n",
            "Sequence length (s): 8192\n",
            "Batch size (b): 16\n",
            "Use FlashAttention: Yes\n",
            "Use a KV cache: Yes\n"
          ]
        }
      ],
      "source": [
        "#Number of parameters in the model (in billions)\n",
        "nb_billion_parameters = 70.6 # @param {type:\"number\"}\n",
        "print(\"Number of parameters in the model (n): \"+str(nb_billion_parameters)+\"B\")\n",
        "\n",
        "#Precision of the parameters in the model\n",
        "bitwidth_model = 16 # @param {type:\"integer\"}\n",
        "print(\"Bitwidth of the model's parameters (p): \"+str(bitwidth_model)+\"-bit\")\n",
        "\n",
        "#The maximum number of tokens in a sequence\n",
        "seqlen = 8192 # @param {type:\"integer\"}\n",
        "print(\"Sequence length (s): \"+str(seqlen))\n",
        "\n",
        "#The batch size\n",
        "batch_size = 16 # @param {type:\"integer\"}\n",
        "print(\"Batch size (b): \"+str(batch_size))\n",
        "\n",
        "\n",
        "\n",
        "#Use FlashAttention\n",
        "Flash_Attention = True # @param {type:\"boolean\"}\n",
        "tile_size = 128\n",
        "if Flash_Attention:\n",
        "  print(\"Use FlashAttention: Yes\")\n",
        "else:\n",
        "  print(\"Use FlashAttention: No\")\n",
        "\n",
        "#Use a KV cache (if yes, should be equal to the seqlen)\n",
        "Use_KV_Cache = True # @param {type:\"boolean\"}\n",
        "kv_cache = 0\n",
        "if Use_KV_Cache:\n",
        "  print(\"Use a KV cache: Yes\")\n",
        "  kv_cache = seqlen\n",
        "else:\n",
        "  print(\"Use a KV cache: No\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to get the estimation given the information provided in the previous cells."
      ],
      "metadata": {
        "id": "9MOm8LVQHm_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def estimate_consumption_inference():\n",
        "  return round((32*seqlen*batch_size*hidden_size + 4*attention_heads*seqlen*seqlen*batch_size)*2/(1000**3),2)\n",
        "def estimate_consumption_inference_gqa():\n",
        "  return round((28*seqlen*batch_size*hidden_size + ((2*kv_heads)/attention_heads)*seqlen*batch_size*hidden_size + 4*kv_heads*seqlen*seqlen*batch_size)*2/(1000**3),2)\n",
        "\n",
        "def estimate_consumption_inference_FA(): #Ignoring GQA for simplicity; will be slightly lower with GQA\n",
        "  return round((32*seqlen*batch_size*hidden_size + 4*tile_size*seqlen*batch_size)*2/(1000**3),2)\n",
        "\n",
        "\n",
        "def kv_cache():\n",
        "  return round(2*hidden_layers*seqlen*batch_size*hidden_size*2/(1000**3),2)\n",
        "\n",
        "def kv_cache_gqa():\n",
        "  return round(2*hidden_layers*seqlen*batch_size*(hidden_size/kv_heads)*2/(1000**3),2)\n",
        "\n",
        "def estimate_model_size():\n",
        "  return round(nb_billion_parameters*bitwidth_model/8*(1000**3)/(1000**3),2)\n",
        "\n",
        "\n",
        "activation_consumption_inference = estimate_consumption_inference()\n",
        "activation_consumption_inference_gqa = estimate_consumption_inference_gqa()\n",
        "activation_consumption_inference_FA = estimate_consumption_inference_FA()\n",
        "model_consumption = estimate_model_size()\n",
        "\n",
        "print(\"Memory consumption of the model: \"+str(model_consumption)+\" GB\\n\")\n",
        "\n",
        "print(\"Memory consumption of vanilla inference: \"+str(activation_consumption_inference)+\" GB \\n\")\n",
        "if kv_heads > 0:\n",
        "  print(\"Memory consumption of inference with GQA: \"+str(activation_consumption_inference_gqa)+\" GB \\n\")\n",
        "\n",
        "print(\"Memory consumption of inference with FlashAttention: \"+str(activation_consumption_inference_FA)+\" GB \\n\")\n",
        "\n",
        "if Use_KV_Cache:\n",
        "  if kv_heads > 0:\n",
        "    kv_cache_cost = kv_cache_gqa()\n",
        "    print(\"Memory consumption of the KV cache (with GQA): \"+str(kv_cache_cost)+\" GB \\n\")\n",
        "  else:\n",
        "    kv_cache_cost = kv_cache()\n",
        "    print(\"Memory consumption of the KV cache: \"+str(kv_cache_cost)+\" GB \\n\")\n",
        "else:\n",
        "  kv_cache_cost = 0\n",
        "\n",
        "if Flash_Attention:\n",
        "  print(\"Total Memory consumption (given the selected configuration): \"+str(round(model_consumption+kv_cache_cost+activation_consumption_inference_FA,2))+\" GB\\n\")\n",
        "elif kv_heads > 0:\n",
        "  print(\"Total Memory consumption (given the selected configuration): \"+str(round(model_consumption+kv_cache_cost+activation_consumption_inference_gqa,2))+\" GB\\n\")\n",
        "else:\n",
        "  print(\"Total Memory consumption (given the selected configuration): \"+str(round(model_consumption+kv_cache_cost+activation_consumption_inference,2))+\" GB\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgSFQfZcOEKx",
        "outputId": "90bdf254-7a7a-4e19-85a5-a37caeb7230c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory consumption of the model: 141.2 GB\n",
            "\n",
            "Memory consumption of vanilla inference: 618.48 GB \n",
            "\n",
            "Memory consumption of inference with GQA: 129.39 GB \n",
            "\n",
            "Memory consumption of inference with FlashAttention: 68.85 GB \n",
            "\n",
            "Memory consumption of the KV cache (with GQA): 42.95 GB \n",
            "\n",
            "Total Memory consumption (given the selected configuration): 253.0 GB\n",
            "\n"
          ]
        }
      ]
    }
  ]
}